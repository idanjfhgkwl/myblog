---
title: "불균형 분류에 대한 평가 지표 둘러보기 (번역)"
categories:
  - study
output: 
  html_document:
    keep_md: true
---

출처: Jason Brownlee, Tour of Evaluation Metrics for Imbalanced Classification, Machine Learning Mastery, 2020.01.08
https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/

<details markdown="1">
<summary>접기/펼치기</summary>

<!--summary 아래 빈칸 공백 두고 내용을 적는공간-->

# 불균형 분류에 대한 평가 지표 둘러보기
(Tour of Evaluation Metrics for Imbalanced Classification)

분류기는 평가에 사용되는 측정 항목만큼만 우수합니다.  
<br>
모델을 평가하기 위해 잘못된 메트릭을 선택하면 불량 모델을 선택하거나 최악의 경우 모델의 예상 성능에 대해 오해 할 가능성이 있습니다.  
<br>
적절한 측정 항목을 선택하는 것은 일반적으로 응용 기계 학습에서 어렵지만 불균형 분류 문제의 경우 특히 어렵습니다. 첫째, 널리 사용되는 대부분의 표준 메트릭은 균형 잡힌 클래스 분포를 가정하고 일반적으로 모든 클래스가 아니므로 모든 예측 오류가 아닌 불균형 분류에 대해 동일하기 때문입니다.  
<br>
이 자습서에서는 불균형 분류에 사용할 수있는 메트릭을 발견합니다.  
이 자습서를 완료하면 다음을 알게됩니다.  
- 분류를 위한 메트릭 선택의 문제 및 편향된 클래스 분포가있을 때 특히 어떻게 어려운지에 대해 설명합니다.
- 등급, 임계 값 및 확률이라고하는 분류기 모델을 평가하기위한 세 가지 주요 메트릭 유형이있는 방법
- 어디서부터 시작해야할지 모르는 경우 불균형 분류에 대한 메트릭을 선택하는 방법.  

모든 예제에 대한 단계별 자습서 및 Python 소스 코드 파일을 포함하여 저의 새로운 저서 [Imbalanced Classification with Python](https://machinelearningmastery.com/imbalanced-classification-with-python/)으로 프로젝트 를 시작하십시오 .

## 평가 지표의 과제
(Challenge of Evaluation Metrics)

평가 메트릭은 예측 모델의 성능을 정량화합니다.  
<br>
여기에는 일반적으로 데이터 세트에서 모델을 학습시키고, 모델을 사용하여 학습 중에 사용되지 않은 홀드 아웃 데이터 세트에 대한 예측을 수행 한 다음 예측을 홀드 아웃 데이터 세트의 예상 값과 비교합니다.  
<br>
분류 문제의 경우 메트릭에는 예상 클래스 레이블을 예측 된 클래스 레이블과 비교하거나 문제에 대한 클래스 레이블의 예측 확률을 해석하는 작업이 포함됩니다.  
<br>
모델을 선택하고 데이터 준비 방법을 함께 사용하는 것은 평가 메트릭에 의해 안내되는 검색 문제입니다. 실험은 다른 모델로 수행되며 각 실험의 결과는 측정 항목으로 정량화됩니다.

> 평가 측정은 분류 성능을 평가하고 분류 자 ​​모델링을 안내하는 데 중요한 역할을합니다. — [불균형 데이터 분류 : 검토](https://www.worldscientific.com/doi/abs/10.1142/S0218001409007326) , 2009.  

분류 정확도 또는 분류 오류와 같은 분류 예측 모델을 평가하는 데 널리 사용되는 표준 메트릭이 있습니다.  
<br>
표준 메트릭은 대부분의 문제에서 잘 작동하므로 널리 채택됩니다. 그러나 모든 메트릭은 문제 또는 문제에서 중요한 것에 대해 가정합니다. 따라서 사용자 또는 프로젝트 이해 관계자가 모델 또는 예측에 대해 중요하다고 생각하는 것을 가장 잘 포착하는 평가 지표를 선택해야하므로 모델 평가 지표를 선택하기가 어렵습니다.  
<br>
이 문제는 클래스 분포에 왜곡이있을 때 더욱 어려워집니다. 그 이유는 소수 클래스와 다수 클래스 간의 비율이 1 : 100 또는 1 : 1000과 같이 클래스가 불균형하거나 심각하게 불균형 할 때 많은 표준 메트릭이 신뢰할 수 없거나 오해의 소지가 있기 때문입니다.

> 클래스 불균형의 경우, 데이터가 왜곡 될 때 왜곡되지 않은 데이터에 사용되는 상대적으로 강력한 기본 절차가 비참하게 무너질 수 있기 때문에 문제는 훨씬 더 심각합니다. — 페이지 187, [불균형 학습 : 기초, 알고리즘 및 응용 프로그램](https://amzn.to/32K9K6d) , 2013.  

예를 들어, 심각하게 불균형 한 분류 문제에 대한 분류 정확도를보고하는 것은 위험 할 정도로 오해의 소지가 있습니다. 프로젝트 이해 관계자가 결과를 사용하여 결론을 도출하거나 새로운 프로젝트를 계획하는 경우입니다.

> 실제로 불균형 도메인에서 공통 메트릭을 사용하면 최적이 아닌 분류 모델로 이어질 수 있으며 이러한 측정은 왜곡 된 도메인에 민감하지 않기 때문에 잘못된 결론을 내릴 수 있습니다. — [불균형 분포 하에서의 예측 모델링 조사](https://arxiv.org/abs/1505.01658) , 2015.

중요한 것은 불균형 분류로 작업 할 때 종종 다른 평가 지표가 필요하다는 것입니다.  
<br>
모든 클래스를 동등하게 중요하게 취급하는 표준 평가 메트릭과 달리 불균형 분류 문제는 일반적으로 소수 클래스의 분류 오류를 다수 클래스의 분류 오류보다 더 중요하게 평가합니다. 소수 클래스에 초점을 맞춘 성능 메트릭이 필요할 수 있으며, 이는 효과적인 모델을 훈련하는 데 필요한 관찰이 부족한 소수 클래스이기 때문에 어렵습니다.

> 불균형 데이터 세트의 주된 문제는 사용 가능한 데이터 샘플에서 제대로 표현되지 않은 케이스의 성능에 대한 사용자 선호도 편향과 종종 관련된다는 사실에 있습니다. — [불균형 분포 하에서의 예측 모델링 조사](https://arxiv.org/abs/1505.01658) , 2015.

이제 모델 평가 메트릭을 선택하는 문제에 익숙해 졌으므로 선택할 수있는 여러 메트릭의 몇 가지 예를 살펴 보겠습니다.

## 분류 자 평가 지표의 분류
(Taxonomy of Classifier Evaluation Metrics)

분류기 모델을 평가할 때 선택할 수있는 메트릭은 수십 가지가 있으며 학계에서 제안한 메트릭의 모든 애완 동물 버전을 고려할 경우 수백 가지가 있습니다.  
<br>
선택할 수있는 메트릭을 처리하기 위해 [Cesar Ferri](http://personales.upv.es/ceferra/) 등이 제안한 분류법을 사용합니다 . " [분류를위한 성능 측정의 실험적 비교](https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687) "라는 제목의 2008 년 논문에서 . 2013 년 책“ [불균형 학습](https://amzn.to/32K9K6d) ” 에서도 채택되었으며 유용하다고 생각합니다.  
<br>
평가 지표를 세 가지 유용한 그룹으로 나눌 수 있습니다. 그들은:
- 임계 값 메트릭
- 순위 지표
- 확률 메트릭.

이 구분은 일반적으로 분류 자에 대해 실무자가 사용하는 상위 메트릭, 특히 불균형 분류가 분류 체계에 깔끔하게 적합하기 때문에 유용합니다.

> 여러 기계 학습 연구자들이 분류 맥락에서 사용되는 세 가지 평가 지표 제품군을 식별했습니다. 이들은 임계 메트릭 (예 : 정확도 및 F- 측정), 순위 지정 방법 및 메트릭 (예 : 수신기 작동 특성 (ROC) 분석 및 AUC), 확률 적 메트릭 (예 : 평균 제곱근 오차)입니다. — 페이지 189, [불균형 학습 : 기초, 알고리즘 및 응용 프로그램](https://amzn.to/32K9K6d) , 2013.

차례로 각 그룹을 자세히 살펴 보겠습니다.


### 불균형 분류에 대한 임계 값 메트릭
(Threshold Metrics for Imbalanced Classification)

## How to Choose an Evaluation Metric

</details>